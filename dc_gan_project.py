# -*- coding: utf-8 -*-
"""DC-GAN - Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dvkyijt6ts0sZFje3Os09MY7sUqiNQkv

# **Deep Convolutional - Generative Adversarial Network (DC- GAN)**
The Deep Convolutional Generative Adversarial Network (DC-GAN) is a type of AI algorithm used in machine learning and computer vision. It generates realistic data, like images, audio, and text. DC-GAN uses deep convolutional neural networks (CNNs) for both the generator and discriminator models. The generator creates images by learning from convolutional layers that capture spatial relationships. The discriminator distinguishes between real and generated images using similar convolutional layers. During training, the generator and discriminator compete, with the generator trying to produce more realistic images and the discriminator becoming better at identifying real and generated images. DC-GANs have been successful in generating high-quality images and have applications in various domains like image synthesis, style transfer, super-resolution, and image inpainting.
<br><br><br><hr>
###### **This  assignment task is to build a DC-GAN that can generate speech for a single word, in different voices and speaking styles**
###### **<small>PROJECT BY:</small>**<small> Ike Ebubechukwu </small>
###### **<small>COURSE NAME:</small>** <small> Applied Natural Language Processing</small>
###### **<small>COURSE CODE:</small>** <small> ARI5121 </small>
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Importing Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import os
import imageio
import librosa
import librosa.display
import scipy.io.wavfile
import numpy as np
import pandas as pd
from keras.models import Sequential, Model
from keras.layers.convolutional import Conv2D, UpSampling2D,  MaxPooling2D
from keras.layers import LeakyReLU, Input, Dense, Reshape, Flatten, Dropout, Activation, Reshape, Conv2DTranspose
from keras.layers import  BatchNormalization, ZeroPadding2D
from keras.optimizers import Adam
from numpy.random import randn
from numpy.random import choice
import matplotlib.pyplot as plt
# %matplotlib inline

path = '/content/drive/My Drive/Speech-Processing/DC_GAN-Project/'
highest_db = 80.0
mel_spec_frame_size = 512  # 512 samples in frame = 512/fs milliseconds per frame
n_mels = 64  # Number of mel spectrogram filters

"""### Loading the .wav file using librosa"""

def get_wav_files_in_path(data_path = path + 'data/0/'):
    """
    Returns the list of .wav files in a directory.
    :param datapath: Directory to search for wav files in.
    :return: List of paths to wav files.
    """
    files = os.listdir(data_path)
    files_wav = [i for i in files if i.endswith('.wav')]
    return files_wav

os.listdir('/content/drive/My Drive/Speech-Processing/DC_GAN-Project/data/')

"""#### Converting to a mel spectogram
###### Mel bins = 64
"""

def get_melspec_from_wav(wavfile, n_mels=64, plot=False):
    """
    Given a path to a wav file, returns a melspectrogram array.
    np.ndarray [shape=(n_mels, t)]
    :param wavfile: The input wav file.
    :param n_mels: The number of mel spectrogram filters.
    :param plot: Flag to either plot the spectorgram or not.
    :return: Returns a tuple of np.ndarray [shape=(n_mels, t)] and fs
    """
    sig, fs = librosa.load(wavfile,sr=None)

    # Normalize audio to between -1.0 and +1.0
    sig /= np.max(np.abs(sig), axis=0)

    if len(sig) < fs: # pad if less than a second
        shape = np.shape(sig)
        padded_array = np.zeros(fs)
        padded_array[:shape[0]] = sig
        sig = padded_array

    melspec = librosa.feature.melspectrogram(y=sig,
                                             sr=fs,
                                             center=True,
                                             n_fft=mel_spec_frame_size,
                                             hop_length=int(mel_spec_frame_size/2),
                                             n_mels=n_mels)

    if plot:
        plt.figure(figsize=(8, 6))
        plt.xlabel('Time')
        plt.ylabel('Mel-Frequency')
        librosa.display.specshow(librosa.power_to_db(melspec, ref=np.max),
                                 y_axis='mel',
                                 fmax=fs/2,
                                 sr=fs,
                                 hop_length=int(mel_spec_frame_size / 2),
                                 x_axis='time')
        plt.colorbar(format='%+2.0f dB')
        plt.title('Mel spectrogram')
        plt.tight_layout()
        plt.show()

    return melspec, fs

"""##### saving as both .PNG files and .NPZ files"""

n_mels = 64
plot = False
wavfile_path = path + 'data/0/'
output_path = path + 'data/Output'

os.makedirs(output_path, exist_ok=True)  # Create output directory if it doesn't exist

for filename in os.listdir(wavfile_path):
    if filename.endswith('.wav'):
        wavfile = os.path.join(wavfile_path, filename)
        melspec, fs = get_melspec_from_wav(wavfile, plot=plot)
        # Process the melspec and fs for each file as needed
        
        # Save as NPZ file
        npzfile = os.path.join(output_path, filename.replace('.wav', '.npz'))
        np.savez(npzfile, melspec=melspec, fs=fs)
        
        # Save as PNG file without whitespace
        pngfile = os.path.join(output_path, filename.replace('.wav', '.png'))
        
        fig, ax = plt.subplots(figsize=(8, 6))
        librosa.display.specshow(librosa.power_to_db(melspec, ref=np.max), y_axis='mel', fmax=fs / 2, sr=fs)
        ax.axis('off')
        plt.tight_layout(pad=0)
        plt.savefig(pngfile, bbox_inches='tight', pad_inches=0)
        plt.close(fig)

"""#### Reconstructing the training data back into audio, to check the audio quality"""

fs

def reconstruct_wav_from_melspec(melspec, fs, plot=False):
    """
    Given a mel-spectrogram, and a target sampling frequency, reconstructs audio.
    :param melspec: Mel-spectrogram, np.ndarray [shape=(n_mels, t)]
    :param fs: Sampling frequency
    :param plot: Flag to either plot the wav or not.
    :return: The reconstructed raw samples of an audio signal.
    """

    #convert normed image back to dB range
    melspec = librosa.power_to_db(melspec, ref= np.max)

    # convert db back to power
    melspec = librosa.db_to_power(melspec)

    sig = librosa.feature.inverse.mel_to_audio(M=melspec,
                                               sr=fs,
                                               center=True,
                                               n_fft=mel_spec_frame_size,
                                               hop_length=int(mel_spec_frame_size/2))

    # Normalize audio to between -1.0 and +1.0
    sig /= np.max(np.abs(sig), axis=0)

    if len(sig) < fs: # pad if less than a second
        shape = np.shape(sig)
        padded_array = np.zeros(fs)
        padded_array[:shape[0]] = sig
        sig = padded_array

    if plot:
        plt.figure(figsize=(10, 4))
        plt.xlabel('Sample')
        plt.ylabel('Amplitude')
        plt.plot(sig)
        plt.title('Waveform')
        plt.tight_layout()
        plt.show()

    return sig

melspec

# Call the reconstruct_wav_from_melspec function
reconstructed_audio = reconstruct_wav_from_melspec(melspec, fs, plot=True)
reconstructed_audio

"""#### Saving audio for playback/listening tests"""

import scipy.io.wavfile as wavfile

# Assuming you have the reconstructed audio array stored in the variable 'audio'
output_path = path + 'data/reconstructed_audio.wav'  # Set the desired output path

# Convert the audio samples to the appropriate data type (int16) for saving as WAV
audio = (reconstructed_audio * 32767).astype(np.int16)

# Save the audio to disk
wavfile.write(output_path, fs, audio)

"""###  (Deep Convolutional Generative Adversarial Network) DCGAN Design

The code below retrieves sub-folders within a given data path, assuming each sub-folder represents a separate class, and assigns numeric class labels to them.
"""

import pathlib

def get_classes_in_datapath(datapath= path + 'data/'):
    """
    Returns a list of sub-folders in the data path, assuming each subfolder is a separate class. Each sub-folder
    is associated with a numeric class label, which is also returned.
    :param datapath: Main data path
    :return: Tuple of individual class folder paths, and the corresponding numeric class label for each folder
    """
    subfolders = [f.path for f in os.scandir(datapath) if f.is_dir() and pathlib.Path(f.path).name.isdigit() ]
    class_labels = np.arange(0,len(subfolders))
    return subfolders, class_labels

classes = get_classes_in_datapath()
classes

"""
The code below retrieves a list of .npz files present in a specified directory by filtering the files based on their file extension."""

def get_npz_files_in_path(datapath = path + '/data/Output/'):
    """
    Returns the list of .npz files in a directory.
    :param datapath: Directory to search for npz files in
    :return: List of paths to npz files
    """
    files = os.listdir(datapath)
    files_npz = [i for i in files if i.endswith('.npz')]
    return files_npz

npz_files = get_npz_files_in_path()
#npz_files

"""The code below saves the current image, represented by a `fig` object, without any whitespace or axes, to a specified file path. It adjusts the plot parameters, removes axes and margins, and saves the figure with tight bounding box and no padding."""

def save_image(filepath, fig=None):
    '''Save the current image with no whitespace
    Example filepath: "myfig.png" or r"C:\myfig.pdf"
    '''
    if not fig:
        fig = plt.gcf()

    plt.subplots_adjust(0,0,1,1,0,0)
    for ax in fig.axes:
        ax.axis('off')
        ax.margins(0,0)
        ax.xaxis.set_major_locator(plt.NullLocator())
        ax.yaxis.set_major_locator(plt.NullLocator())
    fig.savefig(filepath, pad_inches = 0, bbox_inches='tight')

"""The code below performs preprocessing on audio files in the specified data path. It extracts Mel spectrograms from the audio files, pads them if necessary, saves the spectrograms as .mel files and their corresponding images as .png files. It also reconstructs the audio from the spectrograms and saves the reconstructed audio files as .wav files."""

def preprocessing():
    """
    Performs initial data preprocessing - extracting Mel spectra for all files and padding them appropriately
    :return:
    """
    subfolders, class_labels = get_classes_in_datapath()

    for folder in subfolders:

        if not os.path.exists(os.path.join(folder, 'recon')):
            os.makedirs(os.path.join(folder, 'recon'))

        files_wav = get_wav_files_in_path(datapath=folder)
        for file in files_wav:
            # get melspec
            melspec, fs = get_melspec_from_wav(wavfile=os.path.join(folder, file),
                                               plot=False,
                                               n_mels=64)

            melspec = librosa.power_to_db(melspec, ref=1.0)
            melspec = melspec / highest_db # scale by max dB

            #check we have 64 time samples (and pad)
            if melspec.shape[1] < 64:
                shape = np.shape(melspec)
                padded_array = np.zeros((shape[0],64))-1
                padded_array[0:shape[0],:shape[1]] = melspec
                melspec = padded_array

            # save melspec
            melspec_filename = (os.path.join(folder, file)).replace('.wav', '.mel')
            np.savez(melspec_filename, melspec=melspec, fs=fs)

            # save melspec image
            melspec_image_filename = (os.path.join(folder, file)).replace('.wav', '.png')
            fig = plt.figure(figsize=(10, 4))
            plt.imshow(melspec, origin='lower')
            plt.tight_layout()
            save_image(filepath=melspec_image_filename,fig=fig)
            plt.close()

            # reconstruct audio
            melspec = melspec[:,:-1] #remove original pad
            signal = reconstruct_wav_from_melspec(melspec=melspec, fs=fs, plot=False)
            recon_wav_filename = (os.path.join(folder,'recon', file))

            # Convert the audio samples to the appropriate data type (int16) for saving as WAV
            audio = (signal * 32767).astype(np.int16)
            
            # Save the audio to disk
            wavfile.write(audio, fs, audio)

"""The below code retrieves a list of training data files in the form of NPZ files by iterating through subfolders in the data path and appending the file paths to a list. It then returns the list of training data file paths."""

def get_training_set_files():
    """
    Returns a list of training data files (NPZ files)
    :return: List of file paths
    """
    training_images = []

    subfolders, class_labels = get_classes_in_datapath()

    for folder in subfolders:
        files_mel = get_npz_files_in_path(datapath=folder)
        for file in files_mel:
            training_images.append(os.path.join(folder, file))

    return training_images

"""The code below defines a DCGAN (Deep Convolutional Generative Adversarial Network) for generating images. It consists of a generator and a discriminator model. The generator generates images from random noise, while the discriminator distinguishes between real and generated images. The code trains the GAN by iteratively training the discriminator and the generator models. It also includes the functions for preprocessing data, generating latent points, and saving generated images and audio samples during the training process."""

# Commented out IPython magic to ensure Python compatibility.
from keras.regularizers import l2

class DCGAN():
    def __init__(self, training_set):
        # Input shape
        self.img_rows = 16
        self.img_cols = 16
        self.channels = 1
        self.img_shape = (self.img_rows, self.img_cols, self.channels)
        self.latent_dim = 32
        self.trainingset = training_set

        lr = 0.0001 # 0.0000001
        beta_1 = 0.5 # 0.5
        optimizer = Adam(lr, beta_1)

        # Build and compile the discriminator
        self.discriminator = self.build_discriminator()
        self.discriminator.compile(loss='binary_crossentropy',
            optimizer=optimizer,
            metrics=['accuracy'])

        # Build the generator
        self.generator = self.build_generator()

        # The generator takes noise as input and generates imgs
        z = Input(shape=(self.latent_dim,))
        img = self.generator(z)

        # For the combined model we will only train the generator
        self.discriminator.trainable = False

        # The discriminator takes generated images as input and determines validity
        valid = self.discriminator(img)

        # The combined model (stacked generator and discriminator)
        # Trains the generator to fool the discriminator
        self.combined = Model(z, valid)
        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)

    # generate points in latent space as input for the generator
    def generate_latent_points(self, latent_dim, n_samples):
        noise = np.random.normal(0, 1, (n_samples, self.latent_dim))
        return noise

    def build_generator(self):
        """
        Defines a Generator network.
        :return:
        """
        depth = 64

        model = Sequential()

        # Input layer
        model.add(Dense(4 * 4 * int(depth*4), input_shape=(self.latent_dim,)))
        model.add(Reshape((4, 4, int(depth*4))))
        model.add(LeakyReLU(alpha=0.2))

        # Upsampling layers
        model.add(UpSampling2D())
        model.add(Conv2D(int(depth*2), kernel_size=5, strides=1, padding='same'))
        model.add(BatchNormalization())
        model.add(LeakyReLU(alpha=0.2))

        model.add(UpSampling2D())
        model.add(Conv2D(int(depth*2), kernel_size=5, strides=1, padding='same'))
        model.add(BatchNormalization())
        model.add(LeakyReLU(alpha=0.2))

        # Output layer
        model.add(Conv2D(1, kernel_size=3, strides=1, padding='same'))
        model.add(Activation('tanh'))

        model.summary()

        noise = Input(shape=(self.latent_dim,))
        img = model(noise)
        return Model(noise, img)



    def build_discriminator(self):
        """
        Defines a Discriminator network.
        :return:
        """
        depth = 16

        model = Sequential()
        model.add(Conv2D(int(depth), kernel_size=3, strides=1, padding='same', input_shape=self.img_shape))
        model.add(LeakyReLU(alpha=0.2))

        model.add(Flatten())
        model.add(Dropout(0.4))
        model.add(Dense(1, activation='sigmoid'))

        model.summary()
        
        img = Input(shape=self.img_shape)
        validity = model(img)

        return Model(img, validity)

    def train(self, epochs, batch_size=64):

        half_batch = int(batch_size/2)

        # Load the dataset and shuffle it
        X_train = np.asarray(self.trainingset)
        np.take(X_train, np.random.permutation(X_train.shape[0]), axis=0, out=X_train)

        # steps per epoch
        steps_per_epoch = int(X_train.shape[0]/half_batch)

        for epoch in range(epochs):
            for step in range(steps_per_epoch):
                # ---------------------
                #  Train Discriminator
                # ---------------------
                # Adversarial ground truths
                valid = np.ones((half_batch, 1))
                fake = np.zeros((half_batch, 1))

                # Select next batch of images (and shuffle indexes)
                idx = np.arange(step*half_batch,(step*half_batch)+half_batch)
                np.take(idx,np.random.permutation(idx.shape[0]),axis=0,out=idx)
                imgs = []
                for file_path in X_train[idx]:
                    npzfile = np.load(file_path)
                    melspec = npzfile['melspec']
                    imgs.append(melspec)
                imgs = np.asarray(imgs)
                imgs = np.expand_dims(imgs, axis=3)


                # Sample noise and generate a batch of new images
                noise = self.generate_latent_points(self.latent_dim, half_batch)
                gen_imgs = self.generator.predict(noise)

                # Train the discriminator (real classified as ones and generated as zeros)
                d_loss_real, d_acc_real = self.discriminator.train_on_batch(imgs, valid)
                d_loss_fake, d_acc_fake = self.discriminator.train_on_batch(gen_imgs, fake)
                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
                d_acc = 0.5 * np.add(d_acc_real, d_acc_fake)


                # ---------------------
                #  Train Generator
                # ---------------------

                # Train the generator (wants discriminator to mistake images as real)
                # Sample noise and generate a batch of new images
                noise = self.generate_latent_points(self.latent_dim, batch_size)
                valid = np.ones((batch_size, 1))
                g_loss = self.combined.train_on_batch(noise, valid)

                # Plot the progress
                print ("Epoch: (%d/%d) Step: (%d/%d) [D: loss_R: %f, loss_F: %f, loss: %f, acc_R: %.2f%%, acc_F: %.2f%%, acc.: %.2f%%] [G: loss: %f]"
#                        % (epoch, epochs-1, step, steps_per_epoch-1, d_loss_real, d_loss_fake, d_loss, d_acc_real*100, d_acc_fake*100, d_acc*100, g_loss))

            # Save generated image samples at every epoch
            self.save_imgs(epoch)

    def save_imgs(self, epoch):
        img_path = path + 'data/images'
        os.makedirs(img_path, exist_ok=True) 

        samples = 10
        noise = self.generate_latent_points(self.latent_dim, samples)
        gen_imgs = self.generator.predict(noise)

        for sample_idx in range(0,samples):
            melspec = gen_imgs[sample_idx, :,:,0]

            fig = plt.figure(figsize=(10, 4))
            plt.imshow(melspec, origin='lower')
            plt.tight_layout()
            save_image(filepath=(img_path + "/epoch_%d_sample_%d.png" % (epoch,sample_idx)), fig=fig)
            plt.close()

            # reconstruct audio
            melspec = melspec[:,:-1] #remove original pad
            signal = reconstruct_wav_from_melspec(melspec=melspec, fs=16000, plot=False)
            filepath=(img_path + "/epoch_%d_sample_%d.wav" % (epoch,sample_idx))

            # Convert the audio samples to the appropriate data type (int16) for saving as WAV
            audio = (signal * 32767).astype(np.int16)

            # Save the audio to disk
            wavfile.write(filepath, fs, audio)


if __name__ == '__main__':
    do_preprocessing = False

    # step 1 - Preprocessing
    if do_preprocessing:
        preprocessing()

    # step 2 - Build GAN
    training_files = get_training_set_files()
    dcgan = DCGAN(training_set = training_files)

    # step 3 - Train GAN
    batch_size = 32
    dcgan.train(epochs=300, batch_size=batch_size)